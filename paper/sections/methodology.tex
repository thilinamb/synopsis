
\section{Methodology}
\label{sec:methodology}

\input{sections/sketch}

\input{sections/geohash} % This needs to be moved as we get the paper structure finalized

\subsection{Coping with High Data Rates: Scaling out}
\label{subsec:scaling-out}
There are two primary approaches to scaling a node that is experiencing high traffic: replication and load migration.   In replication-based scaling, during high data arrival rates, the original node spawns a new node that is responsible for identical spatial scopes as the original. Assimilation of the newly created node involves partitioning inbound streams directed to the original node. The upstream node is responsible for this partitioning, which may be performed in a skewed fashion with the new node receiving a larger portion of the inbound stream.  Alternatively, inbound streams to the original node may also be partitioned in a round-robin fashion between the original and the newly created node.

In targeted load migration, during heavy traffic the original node spawns a new node. However, in this case, particular geospatial scopes are evicted from the original node to the newly created node. The decision about the spatial scopes to be migrated is based on the data arrival rates and the rates at which particular portions of the sketch are being updated.

Replication-based scaling introduces a challenge during query evaluations in that the query must be forwarded to all nodes responsible for a particular scope and the results merged; depending on the nature of these queries (for e.g., correlation analysis and inferential queries) merging of results may be difficult to accomplish without extensive state synchronizations.
%TODO: lines about how downscaling can be difficult in replication settings

In \textsc{Synopsis}, we use targeted load migration for scaling out.
Our implementation closely follows the MAPE loop~\cite{maurer2011revealing} which is comprised of four phases: monitor (M), analyze (A), planning (P) and execution (E).
The monitoring task as shown in Figure~\ref{fig:process-monitor} periodically probes every \textsc{Synopsis} task to gather two performance metrics as part of monitoring phase.
\begin{enumerate}[leftmargin=*]
	\item Length of the backlog: This represents the number of unprocessed messages in the queue. If the \textsc{Synopsis} task cannot keep up with the incoming data rate, then the backlog will grow over time.
	\item Memory pressure: Each Neptune resource is a JVM process which is being allocated a fixed amount of memory. 
	Exceeding these memory limits create memory pressure which may cause extended garbage collection cycles and increased paging activity. 
	This will eventually lead to reduced performance in every  \textsc{Synopsis} task running in the Neptune resource.
	Monitoring task will continuously record the memory utilization by the entire JVM process as well as the memory utilization by individual \textsc{Synopsis} tasks.
\end{enumerate} 

The objective of scaling out is to maintain the stability of each node.
We define stability as the ability to keep up with incoming data rates while incurring a manageable memory pressure.

During the analysis phase, we use threshold based rules~\cite{lorido2012auto} to provide scale out recommendations to \textsc{Synopsis} nodes if necessary.
Currently we rely on a reactive scheme where we evaluate the threshold based rules based on the current observations.
Scaling out recommendations are provided if either of following rules are consistently satisfied for certain number of observations.
\begin{itemize}[leftmargin=*]  
\item Growing backlog - This is an indication that a portion of the load needs to be migrated to a different \textsc{Synopsis} node.
\item High overall memory utilization above a certain threshold (threshold is usually set below the memory limits allowing a capacity buffer for the process to avoid oscillation)
\end{itemize}

Upon receiving a scaling out recommendation from the monitoring task, a \textsc{Synopsis} node executes planning and execution phases.
During the planning phase, it will choose the portion(s) of the region within its current purview to be handed over to another node.
For this task, it relies on meta-data it maintains for each sub region (region corresponding to a longer Geohash string) and a numeric value provided along with the scale out recommendation which is a measure of how much load should be migrated.
This meta data includes the data rate and the timestamp of the last processed message for each sub region.
A \textsc{Synopsis} node updates these meta data with each message it processes.
Often a \textsc{Synopsis} node migrates several prefixes during a single scale out operation.

Only a single scale out operation takes place at a given time in a Neptune process.
This is controlled using a mutual exclusive lock (mutex) located in each Neptune process.
Further, every scaling operation is followed by a stabilization period where no scaling operation takes place and system does not enter the monitoring phase for the next MAPE cycle.
The objective of above constraints is to avoid oscillations in scaling activities.
For instance, if system aggressively scales out in the presence of a memory pressure without allowing a stabilization period, it is possible that the system ends up in a state where it is under provisioned. As a result of this, it will start aggressively scaling in and run again into an over provisioned state.

Figure~\ref{fig:scale-out-protocol} depicts the phases of the scale out protocol.
Once a \textsc{Synopsis} node decides on the sub regions, it initiates the scale out protocol by contacting the deployer.
In this message, it includes a list of preferred \textsc{Synopsis} nodes for the load migration as well as memory requirements and expected message rate for the load.
The preferred node set includes the \textsc{Synopsis} nodes that already holds its sub regions.
The objective is to minimize the number of \textsc{Synopsis} nodes responsible for holding sketches for a given geographical region, because it reduces the number of \textsc{Synopsis} nodes to contact during a query evaluation for that region.
Deployer has an approximate view of the entire system gathered through gossip messages which includes the memory pressure and cumulative backlog information of each node.
Based on this view and the information present in the request, deployer replies back with a set of target \textsc{Synopsis} nodes.
If it cannot find a suitable node out of the existing set, the deployer will launch a new \textsc{Synopsis} node and include its location in the new request.
Upon receiving the response from the deployer, \textsc{Synopsis} node contacts the target node and try to acquire the mutex.
Lock will be granted if no other scaling operations takes place in the Neptune process which holds the \textsc{Synopsis} node and it can accommodate the migrated load.
The second condition is checked again because the deployer may not have most upto-date metrics regarding the target \textsc{Synopsis} node due to the eventual consistent nature of its system view.
If the lock acquisition is failed, another node from the list of attempted.
Else the original \textsc{Synopsis} node will create a pass-through channel and direct traffic towards the target node.
In the same time, it will initiate a state transfer using a different channel in the background.
This will ensure that the state transfer doesn't affect the stream data flow and it happens asynchronously and the protocol ends without waiting for state transfer to complete.
Even if the state transfer is not complete, the target \textsc{Synopsis} node updates its memory utilization metric to account for the pending state transfer. 
Ending protocol within a short period of time is important because it can release mutual exclusive locks in both origin and target \textsc{Synopsis} nodes quickly and participate in other scaling activities soon after the stabilization period.
%
\begin{figure}
    \centerline{\includegraphics[scale=0.55]{figures/scale-out-protocol.png}}
    \caption{Scale out protocol}
    \label{fig:scale-out-protocol}
\end{figure}
%
\subsection{Downscaling}
During Scaling in, \textsc{Synopsis} nodes attempt to take back some of the sub-regions it scaled out previously to other \textsc{Synopsis} nodes.
This ensures better resource utilization in the system in addition to efficient query evaluations by having to contact less number of nodes.
Scaling in operations are also guarded by the same mutual exclusive lock used for scale out (only one scale out or scale in operation takes place at a given time) and followed by a stabilization period.

Monitoring and analysis phases are similar to scaling out scenario except for the obvious change to the threshold based rules.
Now both memory pressure and backlog length metrics should consistently record values below a predefined lower threshold.
When scaling in, we use a less aggressive scheme than scaling out; a single sub region is acquired during a single scale in operation.
Scaling in is more complex than scaling out because it deals with more than one \textsc{Synopsis} node in most cases.
Because at this point, it is possible that further scale out operations have taken place in the scaled out sub region after the initial scale out.
For instance, if node A in Figure~\ref{fig:stream-partitioning} decides to scale in the sub region \emph{DJK}, then it has to work with both nodes C and E.
Scale in protocol starts with a lock acquisition protocol similar to scale out protocol as illustrated in Figure~\ref{fig:scale-in-protocol}.
But it is required to lock the entire subtree where the sketch for the given sub region is distributed across.
As per our example, node A will have to acquire locks for nodes C and E.
Locks are acquired in a top-to-bottom fashion where parent locks itself and then attempts to lock the child.
If lock acquisition is failed in any part of the sub tree, then the scale in operation is aborted and the monitoring process will start the next iteration of the MAPE loop immediately.
If the sub tree is successfully locked, then data flow to the child nodes corresponding to this sub region is immediately terminated.
If there was a short circuit set up before, it will be removed and the data will start flow to the parent node.
The state acquisition phase begins next.
To ensure that \textsc{Synopsis} does not lose any messages, the initiator node sends a termination point control message to the child node.
Termination point is the sequence number of the last message sent to the child node either by the parent itself or by the short circuit channel.
It may be possible that the child has already processed this message and updated its sketch by the time it receives the termination point control message.
But in extreme cases, the termination point control message may get processed before the actual stream packet with the same sequence number.
This is because control plane and data plane use separate channels and also due to the possibility of data plane messages are being queued before processing.
Once the child node has processed every message up to the termination point, it sends out termination point messages to all relevant child nodes (In our example, node C sends a termination point control message to node E upon processing the stream packet corresponding to the termination point sent by node A).
After the entire sub tree has seen all messages up to the termination point, they acknowledge the initiator node and starts transferring their states asynchronously as similar to scale out protocol.
Once the parent node receives acknowledgments from the entire subtree, it starts propagating the protocol end messages to initiate lock releasing.
Locks are released from the bottom to top in the sub tree where parent releases its lock after noticing that every child participated in the scale in protocol has released its lock.

\begin{figure}
    \centerline{\includegraphics[scale=0.55]{figures/scale-in-protocol.png}}
    \caption{Scale in protocol}
    \label{fig:scale-in-protocol}
\end{figure}

\input{sections/queries.tex}

\subsection{Coping with Failures in \textsc{Synopsis}}
\input{sections/fault-tolerance}

