\section{Methodology}
\label{sec:methodology}
Enabling real-time query evaluations over voluminous spatiotemporal data streams requires several individual components that must coordinate processing and scaling activities over a cluster of heterogeneous resources. This includes our distributed sketch, partitioning scheme, scaling logic, and support for fault tolerance.

\input{sections/sketch}

\input{sections/geohash}

\subsection{Coping with High Data Rates: Scaling out}
\label{subsec:scaling-out}
There are two primary approaches to scaling a node that is experiencing high traffic: \emph{replication} and \emph{load migration}. In replication-based scaling, new nodes are spawned during high data arrival rates that are responsible for identical spatial scopes as their originating node. Assimilation of the newly-created node involves partitioning inbound streams directed to the original node. The upstream node is responsible for this partitioning, which may be performed in a skewed fashion with the new node receiving a larger portion of the inbound stream.  Alternatively, inbound streams to the original node may also be partitioned in a round-robin fashion between the original and the newly-created node.

In targeted load migration, particular geospatial scopes are evicted from the original node to the newly created node during heavy traffic. Deciding which spatial scopes to migrate is based on data arrival rates and the rates at which particular portions of the sketch are being updated.

%[malensek] NOTE: I removed this because it contradicts our discussion on the ability of the sketch to merge arbitrary states (even duplicated state -- in fact, that makes things easier). I think there are plenty of other reasons why replication-based scaling is weaker anyway.
%Replication-based scaling introduces a challenge during query evaluations in that the query must be forwarded to all nodes responsible for a particular scope and the results merged; depending on the nature of these queries (for e.g., correlation analysis and inferential queries) merging of results may be difficult to accomplish without extensive state synchronizations.
%TODO: lines about how scaling in can be difficult in replication settings

In \textsc{Synopsis}, we use targeted load migration for scaling out.
Our implementation closely follows the MAPE loop~\cite{maurer2011revealing} which is comprised of four phases: monitor (M), analyze (A), planning (P) and execution (E).
%The monitoring task as shown in Figure~\ref{fig:process-monitor} periodically probes every \textsc{Synopsis} task to gather two performance metrics as part of monitoring phase.
A monitoring task periodically probes every node to gather two performance metrics:
\begin{enumerate}[leftmargin=*]
	\item \textbf{Length of the backlog:} This represents the number of unprocessed messages in the queue. If the \textsc{Synopsis} task cannot keep up with the incoming data rate, then the backlog will grow over time.
	\item \textbf{Memory pressure:} Each node is allocated a fixed amount of memory. 
	Exceeding these memory limits creates memory pressure, which may cause extended garbage collection cycles and increased paging activity. 
	This will eventually lead to reduced performance in every \textsc{Synopsis} task running on the node.
	The monitoring task continously monitors the memory utilization of the JVM process and individual \textsc{Synopsis} tasks and triggers scaling activities accordingly.
\end{enumerate} 

The objective of scaling out is to maintain the \emph{stability} of each node.
We define stability as the ability to keep up with incoming data rates while incurring a manageable memory pressure.  During the analysis phase, we use threshold-based rules \cite{lorido2012auto} to provide scale out recommendations to \textsc{Synopsis} nodes.
We rely on a reactive scheme where the rules are evaluated on current observations.
Scaling out recommendations are provided if either of the following rules are consistently satisfied for a certain number of observations:
\begin{itemize}[leftmargin=*]  
\item Backlog growth, which indicates that a portion of the load needs to be migrated to a different \textsc{Synopsis} node.
\item High overall memory utilization above a threshold, which is usually set below the memory limits to allow a capacity buffer for the process to avoid oscillation.
\end{itemize}

Upon receiving a scaling out recommendation from the monitoring task, the \textsc{Synopsis} node executes the planning and execution phases.
During the planning phase, the node chooses portion(s) of the region within its current purview to be handed over to another node.
For this task, the node relies on metadata it maintains for each subregion (corresponding to longer Geohash strings) and a numeric value provided by the scale out recommendation that measures how much load should be migrated.
This metadata includes the data rate and the timestamp of the last processed message for each subregion.
A \textsc{Synopsis} node updates these metadata records with each message it processes.
Nodes often migrate several prefixes during a scale out operation.

Only a single scale out operation takes place at a given time per node, which is controlled by a mutex lock.
Further, every scaling operation is followed by a stabilization period where no scaling operation takes place and system does not enter the monitoring phase for the next MAPE cycle.
The objective of these constraints is to avoid oscillations in scaling activities; for instance, repetitively scaling out in the presence of memory pressure could result in overprovisioning, which would then lead to recurring scale-in operations.

Figure~\ref{fig:scale-out-protocol} depicts the phases of the scale out protocol with respect to our example in Figure~\ref{fig:stream-partitioning} when node A is scaling out to node D .
Once the \textsc{Synopsis} node decides on subregions to scale, it initiates the scale out protocol by contacting the \emph{deployer} node, which is responsible for launching tasks.
In this message, it includes a list of preferred \textsc{Synopsis} nodes for the load migration as well as memory requirements and expected message rate for the load.
The preferred node set includes the \textsc{Synopsis} nodes that already hold other subregions.
The objective here is to minimize the number of nodes responsible for each geographical region to reduce communication during query evaluations.

The \textsc{Synopsis} deployer component has an approximate view of the entire system gathered through gossip messages, which includes the memory pressure and cumulative backlog information for each node.
Based on this view and the information present in the request, the deployer replies back with a set of target \textsc{Synopsis} nodes.
Only if a suitable node cannot be found from the set of currently active nodes, a new node will be spawned.
Upon receiving a response from the deployer, the node that is scaling out contacts the target node and tries to acquire the mutex.
A lock will be granted if the target can accommodate the load and no other scaling operations are taking place.
If the lock acquisition fails, another node from the list is attempted; otherwise, the original \textsc{Synopsis} node will create a pass-through channel and direct traffic towards the target node.
Once this process is complete, the source node will initiate a state transfer asynchronously using a background channel to ensure the stream data flow is not affected, and update its memory utilization metrics to account for the pending state transfer.
%Completing the scale out protocol quickly is important because it can release mutual exclusive locks in both origin and target \textsc{Synopsis} nodes quickly and participate in other scaling activities soon after the stabilization period.
%
\begin{figure*}
        \begin{subfigure}{0.45\textwidth}
                \centering
                \includegraphics[scale=0.45, valign=t]{figures/scale-out.pdf}
                \caption{Scale out protocol}
                \label{fig:scale-out-protocol}    
        \end{subfigure}
        \begin{subfigure}{0.45\textwidth}
                \centering
                \includegraphics[scale=0.45, valign=t]{figures/scale-in.pdf} 
                \caption{Scale in protocol}
                \label{fig:scale-in-protocol}
        \end{subfigure}
        \caption{Steps of dynamic scaling protocols in chronological order}
        \label{fig:dynamic-scaling-protocols}
\end{figure*}
%
\subsection{Scaling In}
\label{subsec:scaling-in}
During scaling in, \textsc{Synopsis} nodes merge back some of the subregions scaled out previously.
This ensures better resource utilization in the system in addition to efficient query evaluations by having to contact fewer nodes.
Scaling in is also guarded by the same mutex lock used for scaling out (only one scale out or scale in operation takes place at a given time) and are followed by a stabilization period.

Monitoring and analysis during scale-in operations proceeds similarly to scaling out, except for the obvious change to the threshold-based rules: now both memory pressure and backlog length metrics should consistently record values below a predefined lower threshold.
When scaling in, we use a less aggressive scheme than scaling out; a single subregion is acquired during a single scale in operation.
Scaling in is more complex than scaling out because it involves more than one \textsc{Synopsis} node in most cases.
At this point, it is possible that further scale out operations have taken place in the scaled out subregion after the initial scale out.
For instance, if node A in Figure~\ref{fig:stream-partitioning} decides to scale in the subregion \emph{DJK}, then it must communicate with both nodes C and E.

The scale-in protocol starts with a lock acquisition protocol similar to scaling out protocol, but locking the entire subtree is required.
The steps are depicted in Figure~\ref{fig:scale-in-protocol} with respect to our example in Figure~\ref{fig:stream-partitioning} where node C is scaled in.
As per our example, node A will have to acquire locks for nodes C and E.
Locks are acquired in a top-to-bottom fashion where parent locks itself and then attempts to lock the child.
If lock acquisition is failed in any part of the subtree, then the scale in operation is aborted and the monitoring process will start the next iteration of the MAPE loop immediately.
If the subtree is successfully locked, then data flow to the child nodes corresponding to this subregion is immediately terminated.

The state acquisition phase begins next.
To ensure that \textsc{Synopsis} does not lose any messages, the initiating node sends a \emph{termination point} control message to the child node.
The termination point is the sequence number of the last message sent to the child node either by the parent itself or by the short circuit channel.
%It may be possible that the child has already processed this message and updated its sketch by the time it receives the termination point control message, but in extreme cases the termination point control message may get processed before the actual stream packet with the same sequence number.
%This is because control plane and data plane use separate channels and also due to the possibility of data plane messages are being queued before processing.
Once the child node has processed every message up to the termination point, it sends out termination point messages to all relevant child nodes. In our example, node C sends a termination point control message to node E upon processing the stream packet corresponding to the termination point sent by node A.
After the entire subtree has seen all messages up to the termination point, they acknowledge the initiator node and start transferring their states asynchronously.
Once the parent node receives acknowledgments from the entire subtree, it starts propagating the protocol end messages to initiate lock releasing.
Locks are released from the bottom to top in the subtree, with the parent releasing its lock after each child has released its lock.

\input{sections/queries.tex}

\subsection{Coping with Failures in Synopsis}
\input{sections/fault-tolerance}

