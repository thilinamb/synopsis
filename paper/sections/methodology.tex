
\section{Methodology}
\label{sec:methodology}

\input{sections/sketch}

\input{sections/geohash} % This needs to be moved as we get the paper structure finalized

\subsection{Coping with High Data Rates: Scaling out}
\label{subsec:scaling-out}
There are two primary approaches to scaling a node that is experiencing high traffic: replication and load migration.   In replication-based scaling, during high data arrival rates, the original node spawns a new node that is responsible for identical spatial scopes as the original. Assimilation of the newly created node involves partitioning inbound streams directed to the original node. The upstream node is responsible for this partitioning, which may be performed in a skewed fashion with the new node receiving a larger portion of the inbound stream.  Alternatively, inbound streams to the original node may also be partitioned in a round-robin fashion between the original and the newly created node.

In targeted load migration, during heavy traffic the original node spawns a new node. However, in this case, particular geospatial scopes are evicted from the original node to the newly created node. The decision about the spatial scopes to be migrated is based on the data arrival rates and the rates at which particular portions of the sketch are being updated.

Replication-based scaling introduces a challenge during query evaluations in that the query must be forwarded to all nodes responsible for a particular scope and the results merged; depending on the nature of these queries (for e.g., correlation analysis and inferential queries) merging of results may be difficult to accomplish without extensive state synchronizations.
%TODO: lines about how downscaling can be difficult in replication settings

In \textsc{Synopsis}, we use targeted load migration for scaling out.
Our implementation closely follows the MAPE loop~\cite{maurer2011revealing} which is comprised of four phases: monitor (M), analyze (A), planning (P) and execution (E).
The monitoring task as shown in Figure~\ref{fig:process-monitor} periodically probes every \textsc{Synopsis} task to gather two performance metrics as part of monitoring phase.
\begin{enumerate}[leftmargin=*]
	\item Length of the backlog: This represents the number of unprocessed messages in the queue. If the \textsc{Synopsis} task cannot keep up with the incoming data rate, then the backlog will grow over time.
	\item Memory pressure: Each Neptune resource is a JVM process which is being allocated a fixed amount of memory. 
	Exceeding these memory limits create memory pressure which may cause extended garbage collection cycles and increased paging activity. 
	This will eventually lead to reduced performance in every  \textsc{Synopsis} task running in the Neptune resource.
	Monitoring task will continuously record the memory utilization by the entire JVM process as well as the memory utilization by individual \textsc{Synopsis} tasks.
\end{enumerate} 

The objective of scaling out is to maintain the stability of each node.
We define stability as the ability to keep up with incoming data rates while incurring a manageable memory pressure.

During the analysis phase, we use threshold based rules~\cite{lorido2012auto} to provide scale out recommendations to \textsc{Synopsis} nodes if necessary.
Currently we rely on a reactive scheme where we evaluate the threshold based rules based on the current observations.
Scaling out recommendations are provided if either of following rules are consistently satisfied for certain number of observations.
\begin{itemize}[leftmargin=*]  
\item Growing backlog - This is an indication that a portion of the load needs to be migrated to a different \textsc{Synopsis} node.
\item High overall memory utilization above a certain threshold (threshold is usually set below the memory limits allowing a capacity buffer for the process to avoid oscillation)
\end{itemize}

Upon receiving a scaling out recommendation from the monitoring task, a \textsc{Synopsis} node executes planning and execution phases.
During the planning phase, it will choose the portion(s) of the region within its current purview to be handed over to another node.
For this task, it relies on meta-data it maintains for each sub region (region corresponding to a longer Geohash string) and a numeric value provided along with the scale out recommendation which is a measure of how much load should be migrated.
This meta data includes the data rate and the timestamp of the last processed message for each sub region.
A \textsc{Synopsis} node updates these meta data with each message it processes.
Often a \textsc{Synopsis} node migrates several prefixes during a single scale out operation.

Only a single scale out operation takes place at a given time in a Neptune process.
This is controlled using a mutual exclusive lock (mutex) located in each Neptune process.
Further, every scaling operation is followed by a stabilization period where no scaling operation takes place and system does not enter the monitoring phase for the next MAPE cycle.
The objective of above constraints is to avoid oscillations in scaling activities.
For instance, if system aggressively scales out in the presence of a memory pressure without allowing a stabilization period, it is possible that the system ends up in a state where it is under provisioned. As a result of this, it will start aggressively scaling in and run again into an over provisioned state.

Figure~\ref{fig:scale-out-protocol} depicts the phases of the scale out protocol.
Once a \textsc{Synopsis} node decides on the sub regions, it initiates the scale out protocol by contacting the deployer.
In this message, it includes a list of preferred \textsc{Synopsis} nodes for the load migration as well as memory requirements and expected message rate for the load.
The preferred node set includes the \textsc{Synopsis} nodes that already holds its sub regions.
The objective is to minimize the number of \textsc{Synopsis} nodes responsible for holding sketches for a given geographical region, because it reduces the number of \textsc{Synopsis} nodes to contact during a query evaluation for that region.
Deployer has an approximate view of the entire system gathered through gossip messages which includes the memory pressure and cumulative backlog information of each node.
Based on this view and the information present in the request, deployer replies back with a set of target \textsc{Synopsis} nodes.
If it cannot find a suitable node out of the existing set, the deployer will launch a new \textsc{Synopsis} node and include its location in the new request.
Upon receiving the response from the deployer, \textsc{Synopsis} node contacts the target node and try to acquire the mutex.
Lock will be granted if no other scaling operations takes place in the Neptune process which holds the \textsc{Synopsis} node and it can accommodate the migrated load.
The second condition is checked again because the deployer may not have most upto-date metrics regarding the target \textsc{Synopsis} node due to the eventual consistent nature of its system view.
If the lock acquisition is failed, another node from the list of attempted.
Else the original \textsc{Synopsis} node will create a pass-through channel and direct traffic towards the target node.
In the same time, it will initiate a state transfer using a different channel in the background.
This will ensure that the state transfer doesn't affect the stream data flow and it happens asynchronously and the protocol ends without waiting for state transfer to complete.
Even if the state transfer is not complete, the target \textsc{Synopsis} node updates its memory utilization metric to account for the pending state transfer. 
Ending protocol within a short period of time is important because it can release mutual exclusive locks in both origin and target \textsc{Synopsis} nodes quickly and participate in other scaling activities soon after the stabilization period.
%
\begin{figure}
    \centerline{\includegraphics[scale=0.55]{figures/scale-out-protocol.png}}
    \caption{Scale out protocol}
    \label{fig:scale-out-protocol}
\end{figure}
%
\subsection{Downscaling}
During Scaling in, \textsc{Synopsis} nodes attempt to take back some of the sub-regions it scaled out previously to other \textsc{Synopsis} nodes.
This ensures better resource utilization in the system in addition to efficient query evaluations by having to contact less number of nodes.
Scaling in operations are also guarded by the same mutual exclusive lock used for scale out (only one scale out or scale in operation takes place at a given time) and followed by a stabilization period.

Monitoring and analysis phases are similar to scaling out scenario except for the obvious change to the threshold based rules.
Now both memory pressure and backlog length metrics should consistently record values below a predefined lower threshold.
When scaling in, we use a less aggressive scheme than scaling out; a single sub region is acquired during a single scale in operation.
Scaling in is more complex than scaling out because it deals with more than one \textsc{Synopsis} node in most cases.
Because at this point, it is possible that further scale out operations have taken place in the scaled out sub region after the initial scale out.
For instance, if node A in Figure~\ref{fig:stream-partitioning} decides to scale in the sub region \emph{DJK}, then it has to work with both nodes C and E.
Scale in protocol starts with a lock acquisition protocol similar to scale out protocol as illustrated in Figure~\ref{fig:scale-in-protocol}.
But it is required to lock the entire subtree where the sketch for the given sub region is distributed across.
As per our example, node A will have to acquire locks for nodes C and E.
Locks are acquired in a top-to-bottom fashion where parent locks itself and then attempts to lock the child.
If lock acquisition is failed in any part of the sub tree, then the scale in operation is aborted and the monitoring process will start the next iteration of the MAPE loop immediately.
If the sub tree is successfully locked, then data flow to the child nodes corresponding to this sub region is immediately terminated.
If there was a short circuit set up before, it will be removed and the data will start flow to the parent node.
The state acquisition phase begins next.
To ensure that \textsc{Synopsis} does not lose any messages, the initiator node sends a termination point control message to the child node.
Termination point is the sequence number of the last message sent to the child node either by the parent itself or by the short circuit channel.
It may be possible that the child has already processed this message and updated its sketch by the time it receives the termination point control message.
But in extreme cases, the termination point control message may get processed before the actual stream packet with the same sequence number.
This is because control plane and data plane use separate channels and also due to the possibility of data plane messages are being queued before processing.
Once the child node has processed every message up to the termination point, it sends out termination point messages to all relevant child nodes (In our example, node C sends a termination point control message to node E upon processing the stream packet corresponding to the termination point sent by node A).
After the entire sub tree has seen all messages up to the termination point, they acknowledge the initiator node and starts transferring their states asynchronously as similar to scale out protocol.
Once the parent node receives acknowledgments from the entire subtree, it starts propagating the protocol end messages to initiate lock releasing.
Locks are released from the bottom to top in the sub tree where parent releases its lock after noticing that every child participated in the scale in protocol has released its lock.

\begin{figure}
    \centerline{\includegraphics[scale=0.55]{figures/scale-in-protocol.png}}
    \caption{Scale in protocol}
    \label{fig:scale-in-protocol}
\end{figure}

\subsection{Query Evaluations}
\label{subsec:query-eval}
\textsc{Synopsis} incorporates support for user-defined queries that are evaluated over the distributed sketch. Depending on scaling operations and the spatial scope of the queries, evaluations are carried out on one or more \textsc{Synopsis} nodes. Information on the placement of sketch instances in the system and their corresponding feature scopes is maintained at each node in a Geohash prefix tree, with changes propagated through the network in an eventually-consistent manner as data is ingested and scaling maneuvers occur.

The entry point for these queries, called the \emph{conduit}, may be any of the nodes comprising the distributed sketch. During query evaluations, the first step is to identify the set of \textsc{Synopsis} nodes that are relevant to the query. The conduit consults its prefix tree to locate nodes based on spatial, chronological, and feature constraints specified by the user. After this process is complete, the conduit forwards the queries on to the nodes for evaluation and supplies the client with a list of nodes that will respond to the query.

Results from query evaluations performed over the distributed sketch are streamed back to the client.  Precisely what is streamed depends on the nature of the query.  For example, if a client is interested in retrieving average temperatures in Colorado for July 2015 and if there are multiple nodes (within \textsc{Synopsis}) that hold relevant portions of this data; then the query must be forwarded to all these nodes. The results returned from each of these nodes would include {\textless}temperature, frequency{\textgreater} tuples that would then be combined at the client side to compute the final average temperature.

Alternatively, \textsc{Synopsis} nodes participating in the query evaluations will return vertices and edges (from their internal sketches) that satisfied the specified query constraints. The vertices and edges will be restricted to those pertinent to the query. For example, if a query targets a particular feature (say humidity) then vertices/edges relating to other features not targeted by the query will be pruned and not returned.  Spatiotemporal features are exempt from this constraint. As vertices and edges are streamed back to the client from multiple nodes comprising \textsc{Synopsis}, these are organized into a graph that is then used to return the final results. 

The prefix tree also allows query evaluations during both upscaling and downscaling operations. During query evaluations, queries are forwarded to child nodes (created during a recent upscaling operation). When the conduit forwards a query to a \textsc{Synopsis} node and the node is unavailable, it is either due to a failure at that node or a downscaling operation that merges the child node with its parent node. When a node is unreachable, the conduit forwards the query to the parent node. 

\subsection{Query Types}
\textsc{Synopsis} queries can be discrete or continuous. Unlike discrete queries that are evaluated once, continuous queries are evaluated periodically or when observations become available. Though observations are never stored on stable storage, queries specified by users may target spatial and chronological scopes. In the case of continuous queries, the chronological range implicitly evolves as new observations trickle in; in the case of windowing operators, the specified chronological window continually slides over the observational streams. We classify queries -- discrete or continuous -- supported by \textsc{Synopsis} into 5 broad categories, including filter, statistical, density-based, set, and inferential.

\subsubsection{Filter Queries}
specify inequality/relational queries along one or more dimensions. 

\subsubsection{Correlation/statistical queries}
Statistical queries allow users to explore statistical properties of the observational space. For example, users can retrieve and contrast correlations between any two features at different geographic locations at the same time. Alternatively, queries may contrast correlations between different features at different time ranges at the same geographic location. Statistical queries also support retrieval of the mean, standard deviation, and also of feature outliers based on Chebyshev’s rule about the distribution of feature values.

\subsubsection{Kernel Density estimation queries}
Density queries support analysis over the distribution of values associated with a feature over a particular spatiotemporal scope. These include kernel density estimations, estimating the probability of observing a particular value for an observation, and determining the deciles and quartiles for the observed feature. 

\subsubsection{Set Queries}
Set Queries target identification of whether a particular combination of feature values was observed, estimating the cardinality of the dataset, and identifying the frequencies of repeated observations.
\begin{itemize}
	\item Membership (Bloom Filters)
	\item HyperLogLog: probabilistic estimator for approximating the cardinality of a dataset
	\item Count-Min: approximate frequencies of repeated elements within a dataset
\end{itemize}


\subsubsection{Inferential Queries}
Inferential queries are intended to support projections about how the feature is expected to evolve over a spatiotemporal scope. In this study, these projections are predicated on the registration of time-series models at the particular scope. Specifically, we rely on exponential smoothing methods to make forecasts about the estimated values of features at different time points. Besides compact representations and faster evaluations that are amenable to keeping pace with high data arrival rates, smoothing methods are able to handle trends and seasonality that often underpin observational data. We rely on the Holt-Winters model. Here, the trend and seasonality components for the forecast are computed separately and updated independently based on the error computed from the forecasted and observed values. The seasonality component in the Holt-Winters model may either be additive or multiplicative; we work with the multiplicative components that are are known to perform better with real-world applications. 

\subsection{Support for High throughput Query Evaluations}

\subsection{Coping with Failures in \textsc{Synopsis}}
\input{sections/fault-tolerance}

