\textsc{Synopsis} relies on a scheme based on passive replication to recover from node failures.
Other techniques such as active replication increases the resource consumption significantly and it is infeasible to use upstream backups because the state of a \textsc{Synopsis} node depends on the entire set of stream packets it has processed before~\cite{castro2013integrating}.

Support for fault tolerance is implemented by augmenting the data processing graph (which is responsible for updating the distributed sketch) 	with a set of state replication graphs each attached to a \textsc{Synopsis} node.
State replication graph is a two stage graph where stage one operator (\textit{state replication source}) periodically sends changes to the state of its associated \textsc{Synopsis} node since its last message to the stage two operator.
This incremental checkpointing scheme forms an edit stream between the two stages.
This consumes a less bandwidth compared to a periodic checkpointing scheme~ which replicates the entire state every time\cite{castro2013integrating}.
The stage two operator which is also known as the \textit{state replication sink} serializes the messages received via the edit stream to a persistence storage.
By default, \textsc{Synopsis} uses the disk of the machine which hosts the state replication sink operator as the persistence storage.
But necessary API level provisions are included to support high available storage implementations like HDFS~\cite{borthakur2008hdfs}.
Even though there is a state replication source attached to every \textsc{Synopsis} node, there is one state replication sink per Neptune process.
While in operation, state replication sink operator can fan out to more instances to support higher replication levels.
For instance, if the required replication level is 3 then there will be three instances of the state replication sink running on three different Neptune processes.
Having a fixed number of state replication sinks and not using any additional memory to maintain state replicas contribute to a less additional resource footprint required for fault tolerance.

Incremental checkpoints are performed based on a special control message emitted by the stream ingesters.
These messages help to orchestrate a system wide incremental checkpoint.
\textsc{Synopsis} uses upstream backups at stream ingesters to keep a copy of the messages entered the system since the last successful checkpoint.
In case of a failure, all messages since the last checkpoint will be replayed.
\textsc{Synopsis} nodes are implemented as idempotent operators (using the message sequence numbers), hence they will process the replayed messages only if necessary.
Users can use their own policy for defining the period between the incremental checkpoints because too frequent checkpoints can incur high overhead whereas longer periods between successive checkpoints may consume more resources for upstream backups as well as for processing replayed messages in case of a failure.

Membership management in Neptune is implemented using Zookeeper~\cite{hunt2010zookeeper}, which is leveraged to detect failed nodes.
Upon receiving notifications from the system about node failures, an upstream node switches to a secondary child node if necessary.The secondary child node is a \textsc{Synopsis} node running on the process where a state replication sink was running; hence can access the persisted replicated state.
The secondary will start processing messages immediately and start populating its state from the persistence storage in the background.
Due to this mode of operation, there may be a small window of time during which the correctness of queries are affected.
But it will be rectified quickly once the stored state is loaded to the memory and replay of the upstream backup is completed.
Sketch's ability of correctly processing out of order messages and support for merging with other sketches is useful during this failure recovery process.

